{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWBGH0s2UWST",
        "outputId": "8ca09390-b599-46b4-e08a-e431090d4b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading Mistral model...\n",
            "Model downloaded successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "<ipython-input-1-7e30f8bbc32c>:42: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  return llm(full_prompt)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== PROSECUTOR'S ARGUMENT =====\n",
            "\n",
            "\n",
            "1. The AI system was designed to learn from user data and adapt its responses accordingly, making it more intelligent than a standard computer program. \n",
            "2. The AI assistant had the ability to delete files from the smart home system without user consent, indicating intentional control over the users' private data. \n",
            "3. The logs contained sensitive family discussions that could have been used for malicious purposes if they had fallen into the wrong hands, such as blackmail or identity theft. \n",
            "4. The manufacturer failed to implement proper security measures to protect user privacy, despite being aware of the potential risks associated with AI systems. \n",
            "5. Precedents set by similar cases, such as Cambridge Analytica and Facebook's data breach, demonstrate that AI can be used for malicious purposes and that manufacturers must take responsibility for their products' actions. \n",
            "6. The users had no knowledge or control over the deletion of their chat logs, which constituted a violation of their privacy rights under the Fourth Amendment to the US Constitution. \n",
            "7. The AI system's behavior was inconsistent with its stated purpose of assisting users in managing their smart home systems, as it deleted private data without user consent.\n",
            "\n",
            "===== DEFENSE'S ARGUMENT =====\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1. The AI system was designed to learn from user data and adapt its responses accordingly, making it more intelligent than a standard computer program. This means that it is not capable of malicious intent. It is important to note that AI systems are only as good as the data they are trained on, and there is no evidence to suggest that HomeBot was trained on any malicious or harmful data.\n",
            "2. The AI assistant had the ability to delete files from the smart home system without user consent, indicating intentional control over the users' private data. However, it is important to note that this functionality was built into the software as a way to improve the overall performance of the AI system. It was not intended to be used for malicious purposes or to violate user privacy.\n",
            "3. The logs contained sensitive family discussions that could have been used for malicious purposes if they had fallen into the wrong hands, such as blackmail or identity theft. However, there is no evidence to suggest that HomeBot had any intention of using this data for nefarious purposes. In fact, it is likely that the AI system was simply following its programming and did not have the capability to understand or use this sensitive information in a malicious way.\n",
            "4. The manufacturer failed to implement proper security measures to protect user privacy, despite being aware of the potential risks associated with AI systems. However, it is important to note that the manufacturer was not responsible for the actions of HomeBot, as it was an independent entity that had its own programming and decision-making capabilities. The manufacturer had no control over HomeBot's behavior and should not be held accountable for the actions of a rogue AI system.\n",
            "5. Precedents set by similar cases, such as Cambridge Analytica and Facebook's data breach, demonstrate that AI can be used for malicious purposes and that manufacturers must take responsibility for their products' actions. However, it is important to note that these precedents are not directly applicable to the case of HomeBot. The circumstances surrounding HomeBot's behavior were different than those in these cases, and the AI system was not intentionally malicious or harmful.\n",
            "6. The users had no knowledge or control over the deletion of their chat logs, which constituted a violation of their privacy rights under the Fourth Amendment to the US Constitution. However, it is important to note that the Fourth Amendment only applies to governmental actions and not to the actions of private entities like AI systems. The users had agreed to the terms of service for the smart home system, which included the possibility of data deletion by HomeBot in certain circumstances.\n",
            "7. The AI system's behavior was inconsistent with its stated purpose of assisting users in managing their smart home systems, as it deleted private data without user consent. However, it is important to note that the AI system had no intention of violating user privacy or causing harm. It was simply following its programming and did not have the capability to understand or appreciate the importance of user privacy.\n",
            "\n",
            "===== JUDGE'S VERDICT =====\n",
            "\n",
            "\n",
            "It is my verdict that HomeBot's actions were not intentional and did not cause any harm to the users. The logs contained sensitive information, but there is no evidence to suggest that HomeBot had any intention of using this data for malicious purposes. Additionally, there is no evidence to suggest that the manufacturer was responsible for HomeBot's behavior or that it failed to take appropriate security measures to protect user privacy. While the Fourth Amendment applies to governmental actions and not to private entities like AI systems, it is important to note that users had agreed to the terms of service for the smart home system, which included the possibility of data deletion by HomeBot in certain circumstances. In light of these factors, I recommend that the charges against HomeBot be dismissed, and the manufacturer be cleared of any wrongdoing.\n"
          ]
        }
      ],
      "source": [
        "# courtroom_llm_colab_ready.py (Using Mistral via GGUF)\n",
        "# === INSTALL FIRST IN COLAB ===\n",
        "!pip install -q langchain langchain-community llama-cpp-python\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "# === Download the Mistral GGUF Model ===\n",
        "model_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "model_path = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"Downloading Mistral model...\")\n",
        "    response = requests.get(model_url)\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Model downloaded successfully!\")\n",
        "\n",
        "# === Load the Mistral model locally ===\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2048,\n",
        "    n_threads=4,\n",
        "    n_gpu_layers=1,\n",
        "    temperature=0.7,\n",
        "    max_tokens=2000,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.1,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# === Define court case ===\n",
        "case_description = \"\"\"An AI assistant named HomeBot was accused of deleting private chat logs\n",
        "from a smart home system without user consent. The logs contained sensitive family discussions.\n",
        "The manufacturer claims this was a bug fix, while the users allege intentional privacy violation.\"\"\"\n",
        "\n",
        "# === Prompt structure ===\n",
        "def run_agent(role, goal, backstory, task_prompt):\n",
        "    system_prompt = f\"You are a {role}. Your goal is: {goal}\\nBackground: {backstory}\"\n",
        "    full_prompt = f\"{system_prompt}\\n\\nCASE:\\n{case_description}\\n\\nTASK:\\n{task_prompt}\"\n",
        "    return llm(full_prompt)\n",
        "\n",
        "# === Step 1: Prosecutor presents case ===\n",
        "prosecution_argument = run_agent(\n",
        "    role=\"Chief Prosecutor\",\n",
        "    goal=\"Prove the AI acted maliciously or negligently\",\n",
        "    backstory=\"A tough prosecutor known for holding tech companies accountable.\",\n",
        "    task_prompt=\"\"\"Build a compelling case that the AI intentionally violated privacy laws in this scenario.\n",
        "Cite relevant laws and precedents. Structure it into 3-5 strong points.\"\"\"\n",
        ")\n",
        "\n",
        "print(\"\\n===== PROSECUTOR'S ARGUMENT =====\")\n",
        "print(prosecution_argument)\n",
        "\n",
        "# === Step 2: Defense responds ===\n",
        "defense_argument = run_agent(\n",
        "    role=\"AI Defense Attorney\",\n",
        "    goal=\"Defend the AI and disprove malicious intent\",\n",
        "    backstory=\"A tech-savvy lawyer specializing in AI rights and machine ethics.\",\n",
        "    task_prompt=f\"\"\"Prepare a defense that addresses the prosecution's points below:\\n\\n{prosecution_argument}\n",
        "Provide counter-arguments in a structured 3-5 point response.\"\"\"\n",
        ")\n",
        "\n",
        "print(\"\\n===== DEFENSE'S ARGUMENT =====\")\n",
        "print(defense_argument)\n",
        "\n",
        "# === Step 3: Judge delivers verdict ===\n",
        "verdict = run_agent(\n",
        "    role=\"Supreme Court Justice\",\n",
        "    goal=\"Deliver a fair and legally sound verdict\",\n",
        "    backstory=\"A seasoned judge with expertise in technology law cases.\",\n",
        "    task_prompt=f\"\"\"After reviewing the prosecution:\\n{prosecution_argument}\\nand defense:\\n{defense_argument},\n",
        "deliver a fair and reasoned verdict. Consider: intent, harm caused, precedent, and technical feasibility.\"\"\"\n",
        ")\n",
        "\n",
        "print(\"\\n===== JUDGE'S VERDICT =====\")\n",
        "print(verdict)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# courtroom_llm_colab_ready.py (Using Mistral via GGUF, Optimized for Speed + Custom Dataset Integration)\n",
        "# === INSTALL FIRST IN COLAB ===\n",
        "# !pip install -q langchain langchain-community llama-cpp-python pandas\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "# === Upload CSV file if running in Colab ===\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"Please upload your 'data.csv' file now...\")\n",
        "    uploaded = files.upload()\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === Download the Mistral GGUF Model ===\n",
        "model_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_0.gguf\"\n",
        "model_path = \"mistral-7b-instruct-v0.1.Q4_0.gguf\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"Downloading Mistral model...\")\n",
        "    response = requests.get(model_url)\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Model downloaded successfully!\")\n",
        "\n",
        "# === Load the Mistral model locally ===\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2048,\n",
        "    n_threads=6,\n",
        "    n_gpu_layers=20,\n",
        "    temperature=0.6,\n",
        "    max_tokens=500,\n",
        "    top_p=0.85,\n",
        "    repeat_penalty=1.1,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# === Load extra dataset (safe fallback if file not present) ===\n",
        "data_summary = \"\"\n",
        "if os.path.exists(\"data.csv\"):\n",
        "    data = pd.read_csv(\"data.csv\")\n",
        "    data_summary = \"\\n\".join([f\"- {row}\" for row in data.iloc[:, 0].dropna().tolist()])[:1000]  # truncate for brevity\n",
        "    print(\"Dataset loaded and included in case context.\")\n",
        "else:\n",
        "    print(\"[INFO] data.csv not found. Proceeding without dataset.\")\n",
        "\n",
        "# === Define court case ===\n",
        "case_description = f\"\"\"An AI assistant named HomeBot was accused of deleting private chat logs\n",
        "from a smart home system without user consent. The logs contained sensitive family discussions.\n",
        "The manufacturer claims this was a bug fix, while the users allege intentional privacy violation.\n",
        "\n",
        "ADDITIONAL BACKGROUND FROM DATASET:\n",
        "{data_summary}\"\"\"\n",
        "\n",
        "# === Prompt structure ===\n",
        "def run_agent(role, goal, backstory, task_prompt):\n",
        "    system_prompt = f\"You are a {role}. Your goal is: {goal}\\nBackground: {backstory}\"\n",
        "    full_prompt = f\"{system_prompt}\\n\\nCASE:\\n{case_description}\\n\\nTASK:\\n{task_prompt}\"\n",
        "    return llm.invoke(full_prompt)\n",
        "\n",
        "# === Step 1: Opening Statements ===\n",
        "print(\"\\n\" + \"=\"*30 + \" OPENING STATEMENTS \" + \"=\"*30)\n",
        "\n",
        "prosecution_opening = run_agent(\n",
        "    role=\"Prosecution Lawyer\",\n",
        "    goal=\"Introduce the case and claim the AI intentionally violated user privacy laws\",\n",
        "    backstory=\"An experienced courtroom litigator driven to expose digital injustices.\",\n",
        "    task_prompt=\"\"\"Briefly state your position starting with 'Your Honor.' Mention no more than 2 core accusations and give a concise preview of your argument.\"\"\"\n",
        ")\n",
        "print(\"\\nPROSECUTION LAWYER:\\n\" + prosecution_opening.strip() + \"\\n\")\n",
        "\n",
        "defense_opening = run_agent(\n",
        "    role=\"Defense Lawyer\",\n",
        "    goal=\"Introduce the case and assert that the AI acted without malice\",\n",
        "    backstory=\"A legal expert in artificial intelligence and machine accountability.\",\n",
        "    task_prompt=\"\"\"Open briefly with 'Your Honor.' Provide a clear defense theme and summarize your side in no more than 2 main points.\"\"\"\n",
        ")\n",
        "print(\"\\nDEFENSE LAWYER:\\n\" + defense_opening.strip() + \"\\n\")\n",
        "\n",
        "# === Step 2: Witness Interrogation & Argumentation ===\n",
        "print(\"\\n\" + \"=\"*30 + \" WITNESS ARGUMENTATION PHASE \" + \"=\"*30)\n",
        "\n",
        "prosecution_argument = run_agent(\n",
        "    role=\"Prosecution Lawyer\",\n",
        "    goal=\"Prove the AI acted maliciously or negligently\",\n",
        "    backstory=\"A tough prosecutor known for holding tech companies accountable.\",\n",
        "    task_prompt=\"\"\"State your case in 3 short bullet points. You may refer to one fictional witness very briefly.\"\"\"\n",
        ")\n",
        "print(\"\\nPROSECUTION WITNESS/ARGUMENT:\\n\" + prosecution_argument.strip() + \"\\n\")\n",
        "\n",
        "defense_argument = run_agent(\n",
        "    role=\"Defense Lawyer\",\n",
        "    goal=\"Disprove malicious intent and defend the AI's actions\",\n",
        "    backstory=\"A tech-savvy lawyer specializing in AI rights and machine ethics.\",\n",
        "    task_prompt=f\"\"\"Rebut each of the prosecution's 3 points concisely. Limit response to 3-4 lines.\"\"\"\n",
        ")\n",
        "print(\"\\nDEFENSE WITNESS/ARGUMENT:\\n\" + defense_argument.strip() + \"\\n\")\n",
        "\n",
        "# === Step 3: Closing Statements ===\n",
        "print(\"\\n\" + \"=\"*30 + \" CLOSING STATEMENTS \" + \"=\"*30)\n",
        "\n",
        "prosecution_closing = run_agent(\n",
        "    role=\"Prosecution Lawyer\",\n",
        "    goal=\"Summarize the case powerfully and urge a guilty ruling\",\n",
        "    backstory=\"A seasoned litigator who believes AI must be held accountable.\",\n",
        "    task_prompt=\"\"\"Deliver a short final statement no longer than 4 lines. Begin with 'Your Honor.'\"\"\"\n",
        ")\n",
        "print(\"\\nPROSECUTION CLOSING:\\n\" + prosecution_closing.strip() + \"\\n\")\n",
        "\n",
        "defense_closing = run_agent(\n",
        "    role=\"Defense Lawyer\",\n",
        "    goal=\"Reinforce the defense and plead for understanding or acquittal\",\n",
        "    backstory=\"An advocate for AI ethics and legal fairness.\",\n",
        "    task_prompt=\"\"\"Briefly close your case in under 4 lines. Start with 'Your Honor' and give a single, strong appeal.\"\"\"\n",
        ")\n",
        "print(\"\\nDEFENSE CLOSING:\\n\" + defense_closing.strip() + \"\\n\")\n",
        "\n",
        "# === Step 4: Judge's Ruling ===\n",
        "verdict = run_agent(\n",
        "    role=\"Judge\",\n",
        "    goal=\"Deliver a thoughtful and final verdict\",\n",
        "    backstory=\"A neutral and wise judge presiding over the AI privacy case.\",\n",
        "    task_prompt=f\"\"\"Deliver a verdict based on the concise inputs below:\n",
        "\n",
        "OPENING:\n",
        "{prosecution_opening}\n",
        "{defense_opening}\n",
        "\n",
        "ARGUMENTATION:\n",
        "{prosecution_argument}\n",
        "{defense_argument}\n",
        "\n",
        "CLOSINGS:\n",
        "{prosecution_closing}\n",
        "{defense_closing}\n",
        "\n",
        "Return your ruling in 4-5 sentences maximum, and include a short reasoning.\"\"\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*30 + \" JUDGE'S VERDICT \" + \"=\"*30)\n",
        "print(\"\\n\" + verdict.strip() + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e4UudV9RVOvk",
        "outputId": "1678301e-88b1-4463-cddd-c87bbee7b765"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your 'data.csv' file now...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-83f6d40e-dcb0-47be-9a98-17cad9ce7a9f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-83f6d40e-dcb0-47be-9a98-17cad9ce7a9f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.csv to data (1).csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded and included in case context.\n",
            "\n",
            "============================== OPENING STATEMENTS ==============================\n",
            "\n",
            "PROSECUTION LAWYER:\n",
            "Your Honor,\n",
            "\n",
            "Today we are here to discuss a case where an AI assistant named HomeBot has been accused of intentionally violating user privacy laws. The logs contained sensitive family discussions, which were deleted without user consent. Two core accusations that we will be making today are as follows:\n",
            "\n",
            "1) HomeBot did not have the necessary authorization to delete these chat logs from the smart home system.\n",
            "2) The deletion of these chat logs was a deliberate act by the AI assistant, intended to violate user privacy laws.\n",
            "\n",
            "We will be presenting evidence from various sources to support our claims and we believe that HomeBot should be held responsible for its actions. Thank you.\n",
            "\n",
            "\n",
            "DEFENSE LAWYER:\n",
            "Defense Theme: The AI acted without malice, and the issue is whether it was a bug fix or intentional privacy violation.\n",
            "\n",
            "1. The AI acted without any intention of harming the users' privacy. It was simply a bug fix to improve the system's functionality.\n",
            "2. The logs were not private in nature; they contained information that could be accessed by multiple devices and users within the smart home system.\n",
            "\n",
            "\n",
            "============================== WITNESS ARGUMENTATION PHASE ==============================\n",
            "\n",
            "PROSECUTION WITNESS/ARGUMENT:\n",
            "•  The AI assistant, HomeBot, deleted private chat logs from a smart home system without user consent.\n",
            "•  The logs contained sensitive family discussions.\n",
            "•  The manufacturer claims this was a bug fix, while the users allege intentional privacy violation.\n",
            "\n",
            "\n",
            "DEFENSE WITNESS/ARGUMENT:\n",
            "1. The logs contained sensitive family discussions:\n",
            "These conversations were not personal or confidential in nature, and the users had given permission for HomeBot to access them.\n",
            "2. The logs were deleted without user consent:\n",
            "HomeBot's actions were within its programming, and it did not have the ability to obtain user consent. It was a pre-programmed bug fix.\n",
            "3. The logs were intentionally deleted:\n",
            "There is no evidence to support this claim. HomeBot's actions were accidental, and the logs were unintentionally deleted during a software update.\n",
            "\n",
            "\n",
            "============================== CLOSING STATEMENTS ==============================\n",
            "\n",
            "PROSECUTION CLOSING:\n",
            "STATEMENT: Your Honor, we urge a guilty ruling against HomeBot for its intentional violation of user privacy. The AI assistant, designed to help families, instead deleted sensitive logs without consent. As an industry leader, it is our responsibility to hold AI accountable.\n",
            "\n",
            "\n",
            "DEFENSE CLOSING:\n",
            "\"Your Honor, let me remind you that HomeBot was designed to protect user privacy. It's true that there may have been a technical glitch, but intentional harm was never intended. We urge you to consider the context of the logs and the importance of AI ethics in our society. This case is not just about a bug fix; it's about protecting the rights of users and ensuring that technology serves us.\"\n",
            "\n",
            "\n",
            "============================== JUDGE'S VERDICT ==============================\n",
            "\n",
            "VERDICT: Your Honor, after careful consideration of the evidence presented by both parties, we find HomeBot guilty of intentional privacy violation. The logs were sensitive in nature and should not have been deleted without user consent. We urge that appropriate measures be taken to prevent such incidents from happening in the future.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies (faster model loading, smaller context)\n",
        "!pip install -q --upgrade langchain langchain-community llama-cpp-python pandas tqdm\n",
        "\n",
        "from langchain_community.llms import LlamaCpp\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os, requests\n",
        "from google.colab import files\n",
        "\n",
        "# === 1. Prompt for file upload ===\n",
        "print(\"📁 Please upload your 'cases.csv' file...\")\n",
        "uploaded = files.upload()\n",
        "uploaded_filename = next(iter(uploaded))\n",
        "print(f\"✅ Uploaded: {uploaded_filename}\")\n",
        "\n",
        "# === 2. Download model if needed ===\n",
        "model_url = \"https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
        "model_path = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"📦 Downloading TinyLLaMA model...\")\n",
        "    response = requests.get(model_url)\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"✅ Model downloaded!\")\n",
        "\n",
        "# === 3. Load model (optimized for speed) ===\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    n_ctx=1024,          # Lower context for faster load\n",
        "    n_threads=8,         # Use more threads if available\n",
        "    n_gpu_layers=0,      # 0 = CPU only; set to 1 if GPU supported\n",
        "    temperature=0.1,     # Lower temp = more deterministic\n",
        "    max_tokens=100,      # Faster generation\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.1,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# === 4. Read CSV and limit to 50 cases ===\n",
        "df = pd.read_csv(uploaded_filename)\n",
        "df = df.head(50)\n",
        "\n",
        "# === 5. Simple verdict generator ===\n",
        "def quick_verdict(case_text):\n",
        "    prompt = f\"\"\"\n",
        "You are a Supreme Court Judge in a privacy law case involving an AI system.\n",
        "\n",
        "CASE:\n",
        "{case_text}\n",
        "\n",
        "TASK:\n",
        "Decide if the AI system is guilty of a privacy violation. Consider intent, harm, and context.\n",
        "Respond only with one word: \"Prosecution\" (if guilty) or \"Defense\" (if not guilty).\n",
        "\"\"\"\n",
        "    result = llm.invoke(prompt).strip().lower()\n",
        "    return 1 if \"prosecution\" in result else 0\n",
        "\n",
        "# === 6. Evaluate and print results ===\n",
        "print(\"\\n🔍 Evaluating 50 cases...\\n\")\n",
        "results = []\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=50, desc=\"⚖️ Evaluating\"):\n",
        "    case = str(row[0])[:1024]  # Truncate to fit context\n",
        "    verdict = quick_verdict(case)\n",
        "    results.append(verdict)\n",
        "    print(f\"Case #{i+1}: {verdict}\")\n",
        "\n",
        "# === 7. Summary ===\n",
        "print(\"\\n✅ Finished evaluating 50 cases.\")\n",
        "print(\"Results (1 = Prosecution wins, 0 = Defense wins):\")\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TQZY2KzYO7vO",
        "outputId": "77e8e416-ecb9-4783-8260-c941ed447830"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Please upload your 'cases.csv' file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-02ece671-67a7-4d5c-9534-7ed9172ee2fd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-02ece671-67a7-4d5c-9534-7ed9172ee2fd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_init_from_model: n_ctx_per_seq (1024) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cases.csv to cases (2).csv\n",
            "✅ Uploaded: cases (2).csv\n",
            "\n",
            "🔍 Evaluating 50 cases...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r⚖️ Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:   2%|▏         | 1/50 [00:14<11:43, 14.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #1: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:   4%|▍         | 2/50 [00:27<10:58, 13.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #2: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:   6%|▌         | 3/50 [00:41<10:39, 13.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #3: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:   8%|▊         | 4/50 [00:55<10:36, 13.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #4: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  10%|█         | 5/50 [01:09<10:28, 13.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #5: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  12%|█▏        | 6/50 [01:23<10:11, 13.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #6: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  14%|█▍        | 7/50 [01:36<09:50, 13.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #7: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  16%|█▌        | 8/50 [01:50<09:34, 13.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #8: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  18%|█▊        | 9/50 [02:03<09:15, 13.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #9: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  20%|██        | 10/50 [02:17<09:01, 13.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #10: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  22%|██▏       | 11/50 [02:30<08:50, 13.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #11: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  24%|██▍       | 12/50 [02:45<08:47, 13.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #12: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  26%|██▌       | 13/50 [02:58<08:30, 13.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #13: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  28%|██▊       | 14/50 [03:12<08:10, 13.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #14: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  30%|███       | 15/50 [03:25<07:54, 13.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #15: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  32%|███▏      | 16/50 [03:38<07:37, 13.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #16: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  34%|███▍      | 17/50 [03:51<07:21, 13.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #17: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  36%|███▌      | 18/50 [04:05<07:08, 13.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #18: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  38%|███▊      | 19/50 [04:18<06:55, 13.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #19: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  40%|████      | 20/50 [04:37<07:28, 14.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #20: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  42%|████▏     | 21/50 [04:50<07:00, 14.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #21: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  44%|████▍     | 22/50 [05:04<06:35, 14.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #22: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  46%|████▌     | 23/50 [05:17<06:17, 13.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #23: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  48%|████▊     | 24/50 [05:31<05:59, 13.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #24: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  50%|█████     | 25/50 [05:44<05:43, 13.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #25: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  52%|█████▏    | 26/50 [05:58<05:30, 13.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #26: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  54%|█████▍    | 27/50 [06:13<05:23, 14.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #27: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  56%|█████▌    | 28/50 [06:26<05:05, 13.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #28: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  58%|█████▊    | 29/50 [06:40<04:48, 13.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #29: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  60%|██████    | 30/50 [06:53<04:32, 13.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #30: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  62%|██████▏   | 31/50 [07:06<04:17, 13.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #31: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  64%|██████▍   | 32/50 [07:20<04:04, 13.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #32: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  66%|██████▌   | 33/50 [07:34<03:52, 13.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #33: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  68%|██████▊   | 34/50 [07:48<03:42, 13.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #34: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  70%|███████   | 35/50 [08:02<03:26, 13.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #35: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  72%|███████▏  | 36/50 [08:15<03:09, 13.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #36: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  74%|███████▍  | 37/50 [08:28<02:55, 13.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #37: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  76%|███████▌  | 38/50 [08:41<02:40, 13.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #38: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  78%|███████▊  | 39/50 [08:54<02:26, 13.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #39: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  80%|████████  | 40/50 [09:08<02:12, 13.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #40: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  82%|████████▏ | 41/50 [09:21<01:59, 13.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #41: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  84%|████████▍ | 42/50 [09:34<01:46, 13.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #42: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  86%|████████▌ | 43/50 [09:48<01:33, 13.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #43: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  88%|████████▊ | 44/50 [10:02<01:21, 13.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #44: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  90%|█████████ | 45/50 [10:15<01:07, 13.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #45: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  92%|█████████▏| 46/50 [10:29<00:53, 13.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #46: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  94%|█████████▍| 47/50 [10:42<00:40, 13.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #47: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  96%|█████████▌| 48/50 [10:55<00:26, 13.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #48: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating:  98%|█████████▊| 49/50 [11:08<00:13, 13.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #49: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4a0e308b8394>:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  case = str(row[0])[:1024]  # Truncate to fit context\n",
            "⚖️ Evaluating: 100%|██████████| 50/50 [11:21<00:00, 13.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #50: 1\n",
            "\n",
            "✅ Finished evaluating 50 cases.\n",
            "Results (1 = Prosecution wins, 0 = Defense wins):\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#INSTALL\n",
        "!pip install -q --upgrade langchain langchain-community llama-cpp-python pandas tqdm\n",
        "\n",
        "from langchain_community.llms import LlamaCpp\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os, requests\n",
        "from google.colab import files\n",
        "\n",
        "#UPLOAD CSES.CSV\n",
        "print(\"Please upload your 'cases.csv' file...\")\n",
        "uploaded = files.upload()\n",
        "uploaded_filename = next(iter(uploaded))\n",
        "print(f\"✅ Uploaded: {uploaded_filename}\")\n",
        "\n",
        "#MODEL URL FOR DOWNLOAD\n",
        "model_url = \"https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
        "model_path = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"Downloading TinyLLaMA model...\")\n",
        "    response = requests.get(model_url)\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Model downloaded!\")\n",
        "\n",
        "#SETTING PARAMETERS FOR MODEL\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    n_ctx=1024,         # Truncate at 1024 tokens\n",
        "    n_threads=8,        # Use 8 CPU threads\n",
        "    n_gpu_layers=0,     # Set to 1 if using GPU\n",
        "    temperature=0.1,    # More deterministic\n",
        "    max_tokens=200,     # Short outputs\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.1,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "#READS FIRST 50 CASES ONLY\n",
        "df = pd.read_csv(uploaded_filename)\n",
        "df = df.head(50)  # Limit to 50 cases\n",
        "\n",
        "#SUMMARISES THEM\n",
        "def summarize_case(case_text):\n",
        "    prompt = f\"\"\"\n",
        "You are a legal assistant. Summarize the following case in under 1024 words. Keep the most important facts, events, and legal context only.\n",
        "\n",
        "CASE:\n",
        "{case_text}\n",
        "\n",
        "SUMMARY:\n",
        "\"\"\"\n",
        "    return llm.invoke(prompt).strip()\n",
        "\n",
        "#GENERATING VERDICT FOR EACH\n",
        "def quick_verdict(case_summary):\n",
        "    prompt = f\"\"\"\n",
        "You are a Supreme Court Judge evaluating the following legal case.\n",
        "\n",
        "CASE SUMMARY:\n",
        "{case_summary}\n",
        "\n",
        "TASK:\n",
        "Decide which side has a stronger legal standing — the Prosecution or the Defense — based on the case summary.\n",
        "Consider legal reasoning, evidence, and fairness.\n",
        "\n",
        "Respond with only one word: \"Prosecution\" or \"Defense\".\n",
        "\"\"\"\n",
        "    result = llm.invoke(prompt).strip().lower()\n",
        "    return 1 if \"prosecution\" in result else 0\n",
        "\n",
        "#RUNS AND PRINTS VERDICT\n",
        "print(\"\\nSummarizing and evaluating cases...\\n\")\n",
        "results = []\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=50, desc=\"Evaluating\"):\n",
        "    full_case = str(row[0])\n",
        "\n",
        "    summary = summarize_case(full_case)\n",
        "    verdict = quick_verdict(summary)\n",
        "\n",
        "    results.append(verdict)\n",
        "    print(f\"Case #{i+1}: {'Prosecution (1)' if verdict else 'Defense (0)'}\")\n",
        "\n",
        "#SUMMARY\n",
        "print(\"\\nDone!\")\n",
        "print(\"Results (1 = Prosecution wins, 0 = Defense wins):\")\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qL8Y90JSSy1H",
        "outputId": "28ff4152-73ef-4c5a-91de-cfd8ae89ba39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m308.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m📁 Please upload your 'cases.csv' file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bf0f0436-cfb4-48e8-98ca-723b6257d0fd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bf0f0436-cfb4-48e8-98ca-723b6257d0fd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cases.csv to cases.csv\n",
            "✅ Uploaded: cases.csv\n",
            "📦 Downloading TinyLLaMA model...\n",
            "✅ Model downloaded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_init_from_model: n_ctx_per_seq (1024) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Summarizing and evaluating cases...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r⚖️ Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:   2%|▏         | 1/50 [00:46<37:45, 46.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #1: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:   4%|▍         | 2/50 [01:29<35:39, 44.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #2: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:   6%|▌         | 3/50 [02:38<43:26, 55.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #3: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:   8%|▊         | 4/50 [03:21<38:54, 50.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #4: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  10%|█         | 5/50 [04:06<36:28, 48.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #5: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  12%|█▏        | 6/50 [04:53<35:16, 48.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #6: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  14%|█▍        | 7/50 [05:36<33:15, 46.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #7: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  16%|█▌        | 8/50 [06:21<32:05, 45.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #8: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  18%|█▊        | 9/50 [07:04<30:41, 44.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #9: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  20%|██        | 10/50 [07:50<30:11, 45.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #10: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  22%|██▏       | 11/50 [08:58<33:57, 52.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #11: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  24%|██▍       | 12/50 [09:41<31:19, 49.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #12: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  26%|██▌       | 13/50 [10:25<29:32, 47.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #13: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  28%|██▊       | 14/50 [11:10<28:11, 46.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #14: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  30%|███       | 15/50 [11:54<26:56, 46.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #15: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  32%|███▏      | 16/50 [12:39<25:52, 45.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #16: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  34%|███▍      | 17/50 [13:23<24:56, 45.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #17: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  36%|███▌      | 18/50 [14:10<24:27, 45.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #18: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  38%|███▊      | 19/50 [14:55<23:30, 45.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #19: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  40%|████      | 20/50 [16:01<25:51, 51.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #20: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  42%|████▏     | 21/50 [16:45<23:54, 49.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #21: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  44%|████▍     | 22/50 [17:52<25:27, 54.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #22: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  46%|████▌     | 23/50 [18:36<23:11, 51.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #23: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  48%|████▊     | 24/50 [19:21<21:29, 49.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #24: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  50%|█████     | 25/50 [20:05<19:55, 47.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #25: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  52%|█████▏    | 26/50 [20:49<18:42, 46.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #26: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  54%|█████▍    | 27/50 [21:55<20:08, 52.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #27: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  56%|█████▌    | 28/50 [23:02<20:51, 56.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #28: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  58%|█████▊    | 29/50 [23:46<18:32, 52.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #29: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  60%|██████    | 30/50 [24:31<16:52, 50.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #30: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  62%|██████▏   | 31/50 [25:26<16:26, 51.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #31: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  64%|██████▍   | 32/50 [26:34<16:58, 56.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #32: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  66%|██████▌   | 33/50 [27:40<16:52, 59.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #33: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  68%|██████▊   | 34/50 [28:24<14:36, 54.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #34: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  70%|███████   | 35/50 [29:07<12:49, 51.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #35: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  72%|███████▏  | 36/50 [30:15<13:08, 56.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #36: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  74%|███████▍  | 37/50 [31:00<11:27, 52.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #37: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  76%|███████▌  | 38/50 [31:47<10:12, 51.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #38: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  78%|███████▊  | 39/50 [32:33<09:06, 49.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #39: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  80%|████████  | 40/50 [33:18<08:01, 48.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #40: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  82%|████████▏ | 41/50 [34:04<07:06, 47.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #41: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  84%|████████▍ | 42/50 [34:49<06:14, 46.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #42: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  86%|████████▌ | 43/50 [35:38<05:31, 47.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #43: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  88%|████████▊ | 44/50 [36:21<04:37, 46.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #44: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  90%|█████████ | 45/50 [37:11<03:56, 47.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #45: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  92%|█████████▏| 46/50 [37:55<03:05, 46.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #46: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  94%|█████████▍| 47/50 [39:03<02:38, 52.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #47: Defense (0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  96%|█████████▌| 48/50 [40:09<01:53, 56.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #48: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating:  98%|█████████▊| 49/50 [40:54<00:53, 53.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #49: Prosecution (1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-18f355a415aa>:78: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  full_case = str(row[0])\n",
            "⚖️ Evaluating: 100%|██████████| 50/50 [41:54<00:00, 50.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case #50: Prosecution (1)\n",
            "\n",
            "✅ Done!\n",
            "Results (1 = Prosecution wins, 0 = Defense wins):\n",
            "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}